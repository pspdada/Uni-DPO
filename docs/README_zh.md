<div align="center">

# Uni-DPO: A Unified Paradigm for <br> Dynamic Preference Optimization of LLMs <!-- omit in toc -->

<a href='https://arxiv.org/abs/2506.10054'>
<img src='https://img.shields.io/badge/è®ºæ–‡-Arxiv-purple'></a>
<a href='https://huggingface.co/datasets/psp-dada/Uni-DPO'>
<img src='https://img.shields.io/badge/æ•°æ®é›†-HF-Green'></a>
<a href='https://huggingface.co/collections/psp-dada/uni-dpo'>
<img src='https://img.shields.io/badge/æ¨¡å‹-HF-orange'></a>
<a href='https://huggingface.co/papers/2506.10054'>
<img src='https://img.shields.io/badge/è®¨è®ºåŒº-HF-blue'></a>
<a href='https://github.com/pspdada/Uni-DPO/blob/main/LICENSE'>
<img src='https://img.shields.io/badge/è®¸å¯è¯-Apache_2.0-yellow'></a>

<a href='https://modelscope.cn/datasets/pspdada/Uni-DPO'>
<img src='https://img.shields.io/badge/æ•°æ®é›†-ğŸ¤–ModelScope-pink'></a>
<a href='https://modelscope.cn/collections/pspdada/Uni-DPO'>
<img src='https://img.shields.io/badge/æ¨¡å‹-ğŸ¤–ModelScope-red'></a>

<a href="/README.md">English</a> | <b>ä¸­æ–‡</b>

</div>

## ğŸŠ æ–°é—» <!-- omit in toc -->

- [2026.02.16] ğŸ“– ä»£ç ã€æ•°æ®ä¸æ¨¡å‹å·²å‘å¸ƒï¼
- [2026.01.26] ğŸ‰ æˆ‘ä»¬çš„ Uni-DPO è¢« **ICLR 2026** æ¥æ”¶ï¼

## ğŸš€ æ¦‚è§ˆ <!-- omit in toc -->

**Uni-DPO** æå‡ºä¸€ç§ç»Ÿä¸€çš„åŠ¨æ€åå¥½ä¼˜åŒ–èŒƒå¼ï¼Œç”¨äºåŸºäºåå¥½æ•°æ®è®­ç»ƒå¤§è¯­è¨€æ¨¡å‹ã€‚ä¸åŒäºä»¥å¾€å°†æ‰€æœ‰åå¥½æ ·æœ¬ç­‰åŒå¤„ç†çš„ DPO æ–¹æ³•ï¼ŒUni-DPO åŒæ—¶è€ƒè™‘ï¼š**åå¥½æ•°æ®è‡ªèº«è´¨é‡**ä¸**æ¨¡å‹å­¦ä¹ åŠ¨æ€**ï¼Œä»è€Œå®ç°æ›´æœ‰æ•ˆã€æ›´ç¨³å¥çš„åå¥½å­¦ä¹ ã€‚

**æ ¸å¿ƒä¼˜åŠ¿ï¼š**

- **æ•°æ®è´¨é‡æ„ŸçŸ¥**ï¼šè‡ªé€‚åº”åœ°æå‡é«˜è´¨é‡æ ·æœ¬æƒé‡ï¼Œé™ä½æ¨¡ç³Šæ ·æœ¬å½±å“
- **è®­ç»ƒåŠ¨æ€æ„ŸçŸ¥**ï¼šåŠ¨æ€åœ°å…³æ³¨æ¨¡å‹å°šæœªå­¦ä¼šçš„æ ·æœ¬ï¼Œç¼“è§£è¿‡æ‹Ÿåˆ
- **ç»Ÿä¸€ä¸”è½»é‡**ï¼šæ— ç¼å°†åŒè§†è§’åŠ¨æ€åŠ æƒæœºåˆ¶å’Œæ ¡å‡† NLL æŸå¤±é›†æˆåˆ°æ ‡å‡† DPO è®­ç»ƒæµç¨‹ï¼Œé¢å¤–å¼€é”€æå°

## ğŸ“Œ ç›®å½• <!-- omit in toc -->

- [ğŸ”‘ ä¸»è¦ç‰¹æ€§](#-ä¸»è¦ç‰¹æ€§)
- [ğŸ“š æ•°æ®é›†](#-æ•°æ®é›†)
  - [æ–‡æœ¬ç†è§£](#æ–‡æœ¬ç†è§£)
  - [æ•°å­¦æ¨ç†](#æ•°å­¦æ¨ç†)
  - [å¤šæ¨¡æ€ç†è§£](#å¤šæ¨¡æ€ç†è§£)
- [ğŸ“¦ æ¨¡å‹æƒé‡](#-æ¨¡å‹æƒé‡)
- [ğŸ’» ç¯å¢ƒé…ç½®](#-ç¯å¢ƒé…ç½®)
  - [æ–‡æœ¬ç†è§£](#æ–‡æœ¬ç†è§£-1)
  - [æ•°å­¦æ¨ç†](#æ•°å­¦æ¨ç†-1)
  - [å¤šæ¨¡æ€ç†è§£](#å¤šæ¨¡æ€ç†è§£-1)
- [ğŸ“ å¼•ç”¨](#-å¼•ç”¨)

## ğŸ”‘ ä¸»è¦ç‰¹æ€§

- **é¢å‘åå¥½ä¼˜åŒ–çš„åŒè§†è§’åŠ¨æ€åŠ æƒã€‚**
  Uni-DPO è”åˆå»ºæ¨¡äº†*å“ªäº›æ•°æ®å€¼å¾—å­¦ä¹ *ï¼ˆå†…åœ¨è´¨é‡ï¼‰å’Œ*æ¨¡å‹ä»å­˜åœ¨å“ªäº›å›°éš¾*ï¼ˆå­¦ä¹ åŠ¨æ€ï¼‰ã€‚é€šè¿‡ç»“åˆè´¨é‡æ„ŸçŸ¥æƒé‡å’Œæ€§èƒ½æ„ŸçŸ¥æƒé‡ï¼ŒUni-DPO åœ¨æ•´ä¸ªä¼˜åŒ–è¿‡ç¨‹ä¸­åŠ¨æ€é‡æ–°åˆ†é…è®­ç»ƒç„¦ç‚¹ã€‚

<table align="center">
  <p align="center">
    <img src="/docs/figures/figure1.png" width="80%" />
  </p>
</table>

- **è´¨é‡æ„ŸçŸ¥åŠ æƒè¿‡æ»¤æ¨¡ç³Šçš„åå¥½å¯¹ã€‚**
  åå¥½æ•°æ®çš„å¯é æ€§å·®å¼‚å¾ˆå¤§ã€‚Uni-DPO åˆ©ç”¨åå¥½å›ç­”ä¸æ‹’ç»å›ç­”ä¹‹é—´çš„åˆ†æ•°å·®å€¼ï¼Œä¸ºæ¸…æ™°ã€é«˜è´¨é‡çš„åå¥½å¯¹åˆ†é…æ›´é«˜çš„æƒé‡ï¼ŒåŒæ—¶æŠ‘åˆ¶å˜ˆæ‚æˆ–æ¨¡ç³Šçš„æ ·æœ¬ã€‚

<table align="center">
  <p align="center">
    <img src="/docs/figures/figure2.png" width="80%" />
  </p>
</table>

- **æ€§èƒ½æ„ŸçŸ¥åŠ æƒç¼“è§£è®­ç»ƒè¿‡ç¨‹ä¸­çš„è¿‡æ‹Ÿåˆã€‚**
  å¯¹äºæ¨¡å‹å·²ç»æŒæ¡çš„æ ·æœ¬ï¼Œå³ä½¿å®ƒä»¬è´¨é‡å¾ˆé«˜ï¼Œä¹Ÿå¹¶éæ€»æ˜¯æœ€å…·ä¿¡æ¯é‡çš„ã€‚Uni-DPO å¼•å…¥äº†ä¸€ç§ç±»ä¼¼ç„¦ç‚¹æŸå¤±çš„ç¨³å®šåŒ–æ€§èƒ½æƒé‡ï¼Œå®ƒä¼šé™ä½å·²æ‹Ÿåˆè‰¯å¥½æ ·æœ¬çš„æƒé‡ï¼Œè€Œå¼ºè°ƒé‚£äº›å›°éš¾ä½†ä¿¡æ¯é‡å¤§çš„æ ·æœ¬ï¼Œä»è€Œæœ‰æ•ˆå‡å°‘è¿‡æ‹Ÿåˆã€‚

<table align="center">
  <p align="center">
    <img src="/docs/figures/figure3.png" width="80%" />
  </p>
</table>

- **è§£è€¦æ•°æ®è´¨é‡ä¸å­¦ä¹ éš¾åº¦ã€‚**
  å®è¯åˆ†æè¡¨æ˜ï¼Œæ•°æ®è´¨é‡ï¼ˆåˆ†æ•°å·®å€¼ï¼‰å’Œå­¦ä¹ éš¾åº¦ï¼ˆå¥–åŠ±å·®å€¼ï¼‰ä¹‹é—´çš„ç›¸å…³æ€§å¾ˆå¼±ã€‚Uni-DPO æ˜¾å¼åœ°å¯¹è¿™ç§ä¸åŒ¹é…è¿›è¡Œå»ºæ¨¡ï¼Œç¡®ä¿ä¼˜åŒ–è¿‡ç¨‹åŒæ—¶å—åˆ°è¿™ä¸¤ä¸ªç»´åº¦çš„æŒ‡å¯¼ï¼Œè€Œä¸æ˜¯å•ç‹¬ä¾èµ–å…¶ä¸­ä»»ä½•ä¸€ä¸ªã€‚

<table align="center">
  <p align="center">
    <img src="/docs/figures/figure4.png" width="80%" />
  </p>
</table>

- **åœ¨æ–‡æœ¬ã€æ•°å­¦å’Œå¤šæ¨¡æ€åŸºå‡†æµ‹è¯•ä¸­è¾¾åˆ°ä¸šç•Œé¢†å…ˆæ€§èƒ½ã€‚**
  Uni-DPO åœ¨å¤šç§è®¾ç½®ä¸‹å§‹ç»ˆä¼˜äº DPO å’Œ SimPOã€‚

<table align="center">
  <p align="center">
    <img src="/docs/figures/table1.png" width="80%" />
  </p>
</table>

## ğŸ“š æ•°æ®é›†

æˆ‘ä»¬å‘å¸ƒ [ğŸ¤—](https://huggingface.co/datasets/psp-dada/Uni-DPO) [ğŸ¤–](https://modelscope.cn/datasets/pspdada/Uni-DPO) **Uni-DPO æ•°æ®é›†**ï¼ŒåŒ…å«ä¸‰ç±»è®­ç»ƒæ•°æ®ï¼š_æ–‡æœ¬ç†è§£_ã€_æ•°å­¦æ¨ç†_ã€_å¤šæ¨¡æ€ç†è§£_ã€‚

### æ–‡æœ¬ç†è§£

æ•°æ®é›†ç›®å½•ä¸‹çš„ [ğŸ¤—](https://huggingface.co/datasets/psp-dada/Uni-DPO/tree/main/Textual) `Textual` æ–‡ä»¶å¤¹åŒ…å«*æ–‡æœ¬ç†è§£*ä»»åŠ¡çš„è®­ç»ƒæ•°æ®ï¼Œæ¶µç›– v0.1 å’Œ v0.2 ä¸¤ä¸ªè®­ç»ƒè®¾ç½®ã€‚è‹¥æƒ³è¦è‡ªå·±ç”Ÿäº§è¿™äº›è®­ç»ƒæ•°æ®ï¼Œå¯å‚è€ƒæ­¤[æ–‡æ¡£](/Textual/on_policy_data_gen/README_zh.md)ã€‚

<details>
<summary>ç”Ÿæˆæ•°æ®æµç¨‹</summary>

1. ä¸‹è½½ [ğŸ¤—](https://huggingface.co/datasets/HuggingFaceH4/ultrafeedback_binarized) `HuggingFaceH4/ultrafeedback_binarized` æ•°æ®é›†ï¼›
2. ä½¿ç”¨ `decode.py` ç”Ÿæˆè¾“å‡ºå¹¶ä½¿ç”¨ `post_process.py` æ¸…ç†ï¼›
3. ä½¿ç”¨ `reward_model_annotate.py` è¿›è¡Œæ‰“åˆ†ã€‚

</details>

### æ•°å­¦æ¨ç†

*æ•°å­¦æ¨ç†*è®­ç»ƒæ•°æ®ä½äºæ•°æ®é›†ç›®å½•ä¸‹çš„ [ğŸ¤—](https://huggingface.co/datasets/psp-dada/Uni-DPO/tree/main/Math) `Math` æ–‡ä»¶å¤¹ã€‚è‹¥æƒ³è¦è‡ªå·±ç”Ÿäº§è¿™äº›è®­ç»ƒæ•°æ®ï¼Œå¯å‚è€ƒæ­¤[æ–‡æ¡£](/Math/README_zh.md)å¹¶ä½¿ç”¨æ­¤[è„šæœ¬](/Math/data_generation/generate_data.sh)ã€‚

<details>
<summary>ç”Ÿæˆæ•°æ®æµç¨‹</summary>

1. ä¸‹è½½æ•°å­¦é—®é¢˜æ•°æ®é›† [ğŸ¤—](https://huggingface.co/datasets/RLHFlow/numia_prompt_dpo1) `RLHFlow/numia_prompt_dpo1`ï¼›
2. è¿è¡Œ `gen_samples.py` ä»¥è·å¾—æ¨¡å‹è¾“å‡ºï¼›
3. ä½¿ç”¨è§„åˆ™å¥–åŠ± `verifiable_reward_labeling.py` ä¸è¿‡ç¨‹å¥–åŠ±æ¨¡å‹ `progress_reward_labeling.py` æ‰“åˆ†ï¼›
4. è¿è¡Œ `get_uni_dpo_data.py` æ„å»ºåå¥½å¯¹ã€‚

</details>

è¯„æµ‹æ•°æ®ä½äº [ğŸ¤—](https://huggingface.co/datasets/psp-dada/Uni-DPO/blob/main/Math_eval_data.zip) `Math_eval_data.zip`ã€‚è¯„æµ‹ç»†èŠ‚è§[æ–‡æ¡£](/Math/README_zh.md)ã€‚

### å¤šæ¨¡æ€ç†è§£

*å¤šæ¨¡æ€ç†è§£*çš„è®­ç»ƒæ•°æ®ä½äº [ğŸ¤—](https://huggingface.co/datasets/psp-dada/Uni-DPO/tree/main/Multimodal) `Multimodal` æ–‡ä»¶å¤¹ã€‚ä½¿ç”¨æ–¹å¼è¯·å‚è€ƒ[æ–‡æ¡£](/Multimodal/README_zh.md)ã€‚

## ğŸ“¦ æ¨¡å‹æƒé‡

æˆ‘ä»¬å¼€æºäº†åŸºäº **Uni-DPO** æ–¹æ³•è®­ç»ƒçš„ä¸¤ç§ç‰ˆæœ¬æ¨¡å‹æƒé‡ï¼š**v0.1** ä¸ **v0.2**ï¼Œè¦†ç›–å¤šæ¨¡å‹ç³»åˆ—ï¼ŒåŒ…æ‹¬ Llama3-8Bï¼ŒGemma-2-9B-ITï¼Œä»¥åŠ Qwen2.5ã€‚

| åŸºç¡€æ¨¡å‹                                                                             | è®­ç»ƒæ•°æ®                                                                                                                             | è®­ç»ƒè®¾ç½® |                                                                                            Uni-DPO æ¨¡å‹                                                                                            |
| ------------------------------------------------------------------------------------ | ------------------------------------------------------------------------------------------------------------------------------------ | :------: | :------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------: |
| [ğŸ¤—](https://huggingface.co/princeton-nlp/Llama-3-Base-8B-SFT) Llama-3-8B-Base-SFT   | [ğŸ¤—](https://huggingface.co/datasets/psp-dada/Uni-DPO/blob/main/Textual/UltraFeedback-GPT-4o/train.jsonl)                            |   v0.1   |                [ğŸ¤—](https://huggingface.co/psp-dada/Llama-3-8B-Base-SFT-Uni-DPO) [ğŸ¤–](https://modelscope.cn/models/pspdada/Llama-3-8B-Base-SFT-Uni-DPO) Llama-3-8B-Base-SFT-Uni-DPO                |
| [ğŸ¤—](https://huggingface.co/princeton-nlp/Llama-3-Base-8B-SFT) Llama-3-8B-Base-SFT   | [ğŸ¤—](https://huggingface.co/datasets/psp-dada/Uni-DPO/blob/main/Textual/UltraFeedback-Qwen2_5_72B/train.jsonl)                       |   v0.2   |    [ğŸ¤—](https://huggingface.co/psp-dada/Llama-3-8B-Base-SFT-Uni-DPO-v2-Qwen) [ğŸ¤–](https://modelscope.cn/models/pspdada/Llama-3-8B-Base-SFT-Uni-DPO-v2-Qwen) Llama-3-8B-Base-SFT-Uni-DPO-v2-Qwen    |
| [ğŸ¤—](https://huggingface.co/princeton-nlp/Llama-3-Base-8B-SFT) Llama-3-8B-Base-SFT   | [ğŸ¤—](https://huggingface.co/datasets/psp-dada/Uni-DPO/tree/main/Textual)                                                             |   v0.2   |  [ğŸ¤—](https://huggingface.co/psp-dada/Llama-3-8B-Base-SFT-Uni-DPO-v2-GPT-4) [ğŸ¤–](https://modelscope.cn/models/pspdada/Llama-3-8B-Base-SFT-Uni-DPO-v2-GPT-4) Llama-3-8B-Base-SFT-Uni-DPO-v2-GPT-4   |
| [ğŸ¤—](https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct) Llama-3-8B-Instruct | [ğŸ¤—](https://huggingface.co/datasets/psp-dada/Uni-DPO/blob/main/Textual/Llama-3-8B-Instruct-UltraFeedback-GPT-4o/train.jsonl)        |   v0.1   |                [ğŸ¤—](https://huggingface.co/psp-dada/Llama-3-8B-Instruct-Uni-DPO) [ğŸ¤–](https://modelscope.cn/models/pspdada/Llama-3-8B-Instruct-Uni-DPO) Llama-3-8B-Instruct-Uni-DPO                |
| [ğŸ¤—](https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct) Llama-3-8B-Instruct | [ğŸ¤—](https://huggingface.co/datasets/psp-dada/Uni-DPO/blob/main/Textual/Llama-3-8B-Instruct-UltraFeedback-ArmoRM/train.jsonl)        |   v0.2   | [ğŸ¤—](https://huggingface.co/psp-dada/Llama-3-8B-Instruct-Uni-DPO-v2-ArmoRM) [ğŸ¤–](https://modelscope.cn/models/pspdada/Llama-3-8B-Instruct-Uni-DPO-v2-ArmoRM) Llama-3-8B-Instruct-Uni-DPO-v2-ArmoRM |
| [ğŸ¤—](https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct) Llama-3-8B-Instruct | [ğŸ¤—](https://huggingface.co/datasets/psp-dada/Uni-DPO/blob/main/Textual/Llama-3-8B-Instruct-UltraFeedback-ArmoRM-GPT-4o/train.jsonl) |   v0.2   | [ğŸ¤—](https://huggingface.co/psp-dada/Llama-3-8B-Instruct-Uni-DPO-v2-GPT-4o) [ğŸ¤–](https://modelscope.cn/models/pspdada/Llama-3-8B-Instruct-Uni-DPO-v2-GPT-4o) Llama-3-8B-Instruct-Uni-DPO-v2-GPT-4o |
| [ğŸ¤—](https://huggingface.co/google/gemma-2-9b-it) Gemma2-9B-IT                       | [ğŸ¤—](https://huggingface.co/datasets/psp-dada/Uni-DPO/blob/main/Textual/Gemma-2-9B-IT-UltraFeedback-GPT-4o/train.jsonl)              |   v0.1   |                          [ğŸ¤—](https://huggingface.co/psp-dada/Gemma2-9B-IT-Uni-DPO) [ğŸ¤–](https://modelscope.cn/models/pspdada/Gemma2-9B-IT-Uni-DPO) Gemma2-9B-IT-Uni-DPO                           |
| [ğŸ¤—](https://huggingface.co/Qwen/Qwen2.5-7B) Qwen2.5-7B                              | [ğŸ¤—](https://huggingface.co/datasets/psp-dada/Uni-DPO/blob/main/Textual/UltraFeedback-GPT-4o/train.jsonl)                            |   v0.1   |                             [ğŸ¤—](https://huggingface.co/psp-dada/Qwen2.5-7B-Uni-DPO) [ğŸ¤–](https://modelscope.cn/models/pspdada/Qwen2.5-7B-Uni-DPO) Qwen2.5-7B-Uni-DPO                              |
| [ğŸ¤—](https://huggingface.co/Qwen/Qwen2.5-Math-7B) Qwen2.5-Math-7B                    | [ğŸ¤—](https://huggingface.co/datasets/psp-dada/Uni-DPO/blob/main/Math/Train_Qwen_2_5_math_7B.jsonl)                                   |   v0.1   |                      [ğŸ¤—](https://huggingface.co/psp-dada/Qwen2.5-Math-7B-Uni-DPO) [ğŸ¤–](https://modelscope.cn/models/pspdada/Qwen2.5-Math-7B-Uni-DPO) Qwen2.5-Math-7B-Uni-DPO                      |

## ğŸ’» ç¯å¢ƒé…ç½®

ä¸ºç¡®ä¿ä¸ä¹‹å‰çš„ç ”ç©¶è¿›è¡Œå…¬å¹³çš„å¯¹æ¯”ï¼Œæˆ‘ä»¬å°½å¯èƒ½åœ°è®©è®­ç»ƒå’Œæµ‹è¯•ç¯å¢ƒä¸åŸå§‹å®ç°ä¿æŒä¸€è‡´ã€‚ä»¥ä¸‹æ˜¯å„ä¸ªä»»åŠ¡æ‰€ç”¨ç¯å¢ƒçš„ç®€è¦ä»‹ç»ã€‚

### æ–‡æœ¬ç†è§£

è®­ç»ƒç¯å¢ƒï¼šå‚è§[æ–‡æ¡£](/Textual/README_zh.md)ã€‚

- åŸºäº [SimPO](https://github.com/princeton-nlp/SimPO) ä»“åº“æ„å»º
- ä¾èµ– [alignment-handbook](https://github.com/huggingface/alignment-handbook)å¹¶ä½¿ç”¨ `transformers` åº“ä¸­çš„ `Trainer` ç±»æ¥æ„å»º `UniDPOTrainer` ç±»ï¼Œä»¥å®ç° Uni-DPO è®­ç»ƒã€‚

è¯„æµ‹ç¯å¢ƒï¼šä¸»è®ºæ–‡ä¸­æŠ¥å‘Šçš„æŒ‡æ ‡ä¸¥æ ¼éµå¾ªä»¥ä¸‹å››ä¸ªè¯„æµ‹ç¯å¢ƒï¼š[Arena-Hard-Auto](https://github.com/lmarena/arena-hard-auto)ã€[AlpacaEval2](https://github.com/tatsu-lab/alpaca_eval)ã€[IFEval](https://github.com/google-research/google-research/tree/master/instruction_following_eval)ã€[SedarEval](https://github.com/wwn1233/sedareval)ã€‚é™„å½•ä¸­çš„ä¸‹æ¸¸ä»»åŠ¡è¯„æµ‹åˆ™ä½¿ç”¨ [Language Model Evaluation Harness](https://github.com/EleutherAI/lm-evaluation-harness) çš„é…ç½®ã€‚

### æ•°å­¦æ¨ç†

æˆ‘ä»¬çš„è®­ç»ƒä¸è¯„æµ‹ç¯å¢ƒåŸºäº [Online-DPO-R1](https://github.com/RLHFlow/Online-DPO-R1) ä»“åº“æ„å»ºã€‚è¯¦æƒ…è¯·å‚è§[æ–‡æ¡£](/Math/README_zh.md)ã€‚

- **è®­ç»ƒæ•°æ®æ„å»ºï¼š** ä¾èµ– vLLM è¿›è¡Œæ¨¡å‹éƒ¨ç½²ä¸æ¨ç†
- **è®­ç»ƒï¼š** åŒæ ·ä¾èµ– [alignment-handbook](https://github.com/huggingface/alignment-handbook)ï¼Œå¹¶ä½¿ç”¨ `transformers` çš„ `Trainer` ç±»æ„å»º `UniDPOTrainer` ç±»ä»¥æ‰§è¡Œ Uni-DPO è®­ç»ƒ
- **è¯„æµ‹ï¼š** è¯„æµ‹ä»£ç åŸºäº [simpleRL-reason](https://github.com/hkust-nlp/simpleRL-reason)

### å¤šæ¨¡æ€ç†è§£

éµå¾ª [MM-RLHF](https://github.com/Kwai-YuanQi/MM-RLHF)ã€‚è¯¦æƒ…è¯·å‚è§[æ–‡æ¡£](/Multimodal/README_zh.md)ã€‚

- **è®­ç»ƒï¼š** æˆ‘ä»¬çš„è®­ç»ƒç¯å¢ƒåŸºäº [LlamaFactory](https://github.com/hiyouga/LLaMAFactory) æ„å»ºï¼Œå¹¶æä¾›æœ€å°ä¿®æ”¹ç‰ˆæœ¬åŠå¿…è¦è®­ç»ƒè„šæœ¬
- **è¯„æµ‹ï¼š** æˆ‘ä»¬çš„è¯„æµ‹ç¯å¢ƒåŸºäº [VLMEvalKit](https://github.com/open-compass/VLMEvalKit) æ„å»ºï¼Œå¹¶æä¾›è¿è¡Œè¯„æµ‹æ‰€éœ€è„šæœ¬ä¸å¿…è¦æ–‡æ¡£

## ğŸ“ å¼•ç”¨

å¦‚æœæˆ‘ä»¬çš„æ¨¡å‹/ä»£ç /æ•°æ®/è®ºæ–‡å¯¹æ‚¨æœ‰å¸®åŠ©ï¼Œè¯·å¼•ç”¨æˆ‘ä»¬çš„è®ºæ–‡å¹¶ä¸ºæˆ‘ä»¬ç‚¹ â­ï¸ï¼

```bibtex
@inproceedings{penguni,
  title={Uni-DPO: A Unified Paradigm for Dynamic Preference Optimization of LLMs},
  author={Peng, Shangpin and Wang, Weinong and Tian, Zhuotao and Yang, Senqiao and Xu, Haotian and Zhang, Chengquan and Isobe, Takashi and Hu, Baotian and Zhang, Min and others},
  booktitle={The Fourteenth International Conference on Learning Representations}
}
```

## ğŸ“§ è”ç³»æˆ‘ä»¬ <!-- omit in toc -->

å¦‚æœæ‚¨æœ‰ä»»ä½•é—®é¢˜ã€æ„è§æˆ–å»ºè®®ï¼Œæ¬¢è¿æäº¤ issue æˆ– PRï¼Œå…±åŒæ¨åŠ¨è¯¥æ–¹å‘çš„ç ”ç©¶è¿›å±•ã€‚

## ğŸ™ è‡´è°¢ <!-- omit in toc -->

æ„Ÿè°¢ä»¥ä¸‹é¡¹ç›®æä¾›æ”¯æŒï¼š

- æ•°æ®æ¥æºï¼š[ultrafeedback_binarized](https://huggingface.co/datasets/HuggingFaceH4/ultrafeedback_binarized) / [RLHFlow/numia_prompt_dpo1](https://huggingface.co/datasets/RLHFlow/numia_prompt_dpo1) / [MM-RLHF](https://github.com/Kwai-YuanQi/MM-RLHF)
- è®­ç»ƒæ¡†æ¶ï¼š[SimPO](https://github.com/princeton-nlp/SimPO) / [alignment-handbook](https://github.com/huggingface/alignment-handbook) / [Online-DPO-R1](https://github.com/RLHFlow/Online-DPO-R1) / [LlamaFactory](https://github.com/hiyouga/LLaMAFactory)
- è¯„æµ‹å·¥å…·ï¼š
  - æ–‡æœ¬ç†è§£ï¼š[Arena-Hard-Auto](https://github.com/lmarena/arena-hard-auto) / [AlpacaEval2](https://github.com/tatsu-lab/alpaca_eval) / [IFEval](https://github.com/google-research/google-research/tree/master/instruction_following_eval) / [SedarEval](https://github.com/wwn1233/sedareval) / [Language Model Evaluation Harness](https://github.com/EleutherAI/lm-evaluation-harness)
  - æ•°å­¦æ¨ç†ï¼š[simpleRL-reason](https://github.com/hkust-nlp/simpleRL-reason)
  - å¤šæ¨¡æ€ç†è§£ï¼š[VLMEvalKit](https://github.com/open-compass/VLMEvalKit)

## è®¸å¯è¯ <!-- omit in toc -->

[Apache License 2.0](/LICENSE)
