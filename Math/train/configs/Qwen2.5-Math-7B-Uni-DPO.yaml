### Basic Configuration
run_name: Qwen2.5-Math-7B-Uni-DPO

output_dir: <Your output directory path>
model_name_or_path: <Your path to Qwen2.5-Math-7B model>
ref_model: <Your path to Qwen2.5-Math-7B model>
train_data_path: <Your training data jsonl path>

gradient_checkpointing: true
do_train: true
do_eval: false
bf16: true
sanity_check: False
eot_token: <|im_end|>
report_to: "wandb"
lr_scheduler_type: cosine
max_length: 4096
max_prompt_length: 1000

### Training hyperparameters
num_train_epochs: 2
logging_steps: 1
per_device_train_batch_size: 1
per_device_eval_batch_size: 1
gradient_accumulation_steps: 16 # 8GPU x 16 = 128

### Uni-DPO config
learning_rate: 1.0e-6
loss_type: uni_dpo
beta: 2.0
uni_dpo_qual_eta: 0.7
uni_dpo_tau_ref: 2.0
uni_dpo_perf_gamma: 3.0
uni_dpo_nll_lambda: 0.2
uni_dpo_tau_good: 8.0

dpo_length_norm: true

save_strategy: steps
save_steps: 20
